{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TweetsQnABaseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarauppal/TweetQnA_CPSC532P/blob/master/TweetsQnABaseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gkL_AFYzxNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUklYL6L59Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " !ls /content/gdrive/My\\ Drive/TweetQnA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr2wDAjJ6xby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/gdrive/My Drive/TweetQnA'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk9AxsaK7CIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/TweetQnA')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlZRFS256WQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hKeHsSX1jGp",
        "colab_type": "code",
        "outputId": "70fa71c2-46a5-4c31-bd9b-cfaa4c16b3ed",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-433bd6e7-13a8-4efe-9fe0-5124afb5900e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-433bd6e7-13a8-4efe-9fe0-5124afb5900e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bVaIrVz2J7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('perluniprops')\n",
        "nltk.download('punkt')\n",
        "!pip install colorama\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgFKL0Y22TEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "#out-of-vocabulary words to zero\n",
        "def get_pretrained_embedding(np_embd):\n",
        "    embedding = nn.Embedding(*np_embd.shape)\n",
        "    embedding.weight = nn.Parameter(torch.from_numpy(np_embd).float())\n",
        "    embedding.weight.requires_grad = False\n",
        "    return embedding\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, emb_matrix, dropout_ratio):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = get_pretrained_embedding(emb_matrix)\n",
        "        self.emb_dim = self.embedding.embedding_dim\n",
        "\n",
        "        self.encoder = nn.GRU(self.emb_dim, hidden_dim, 1, batch_first=True,\n",
        "                              bidirectional=True, dropout=dropout_ratio)\n",
        "        self.dropout_emb = nn.Dropout(p=dropout_ratio)\n",
        "\n",
        "    def forward(self, seq, mask):\n",
        "        lens = torch.sum(mask, 1)\n",
        "        lens_sorted, lens_argsort = torch.sort(lens, 0, True)\n",
        "        _, lens_argsort_argsort = torch.sort(lens_argsort, 0)\n",
        "        seq_ = torch.index_select(seq, 0, lens_argsort)\n",
        "        seq_embd = self.embedding(seq_)\n",
        "\n",
        "        packed = pack_padded_sequence(seq_embd, lens_sorted, batch_first=True)\n",
        "        output, _ = self.encoder(packed)\n",
        "        e, _ = pad_packed_sequence(output, batch_first=True)\n",
        "        e = e.contiguous()\n",
        "        e = torch.index_select(e, 0, lens_argsort_argsort)  # B x m x 2l\n",
        "        e = self.dropout_emb(e)\n",
        "        return e\n",
        "\n",
        "class Baseline(nn.Module):\n",
        "    def __init__(self, hidden_dim, emb_matrix, dropout_ratio):\n",
        "        super(Baseline, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.encoder = Encoder(hidden_dim, emb_matrix, dropout_ratio)\n",
        "\n",
        "        self.dropout_att = nn.Dropout(p=dropout_ratio)\n",
        "        self.fc = nn.Linear(4*hidden_dim, hidden_dim)\n",
        "        self.fc_start = nn.Linear(hidden_dim, 1)\n",
        "        self.fc_end = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, q_seq, q_mask, d_seq, d_mask, span=None):\n",
        "        Q = self.encoder(q_seq, q_mask)\n",
        "        D = self.encoder(d_seq, d_mask)\n",
        "\n",
        "        b, m, _ = list(D.size())\n",
        "\n",
        "        # query processing ends\n",
        "        #attention\n",
        "        Q_t = torch.transpose(Q, 1, 2)  # B x 2l x n\n",
        "        A = torch.bmm(D, Q_t)  # B x m x n\n",
        "        A = F.softmax(A, dim=2)  # B x m x n\n",
        "        C = torch.bmm(A, Q) # (B x m x n) X (B x n x 2l) => b x m x 2l\n",
        "        B = torch.cat([C, D], 2) # B x m x 4l\n",
        "        B = self.dropout_att(B)\n",
        "        # attention ends\n",
        "        B_hat = F.relu(self.fc(B.view(-1, 4*self.hidden_dim))) # B*m x l\n",
        "\n",
        "        mask_mult = (1.0 - d_mask.float())*(-1e30)\n",
        "\n",
        "        logit_start = self.fc_start(B_hat).view(-1, m)  # B x m\n",
        "        logit_start = logit_start + mask_mult # B x m\n",
        "        _, start_i = torch.max(logit_start, dim=1)\n",
        "\n",
        "        logit_end = self.fc_end(B_hat).view(-1, m)  # B x m\n",
        "        logit_end = logit_end + mask_mult\n",
        "        _, end_i = torch.max(logit_end, dim=1)\n",
        "\n",
        "        if span is not None:\n",
        "            loss_value = self.loss(logit_start, span[:, 0])\n",
        "            loss_value += self.loss(logit_end, span[:, 1])\n",
        "\n",
        "            loss_value = torch.mean(loss_value)\n",
        "            return loss_value, start_i, end_i\n",
        "        else:\n",
        "            return start_i, end_i\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOG4F0294gUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from data_batcher import get_batch_generator\n",
        "from evaluate import exact_match_score, f1_score\n",
        "from official_eval_helper import get_json_data, generate_answers\n",
        "from pretty_print import print_example\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim import Adam\n",
        "from vocab import get_glove\n",
        "\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahQ2S7dL4mHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WBQFnE58jYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "class Config(object):\n",
        "    pass\n",
        "\n",
        "config = Config()\n",
        "\n",
        "config.data_dir = path+'/squad_like_tweets'\n",
        "config.log_root = path+'/log'\n",
        "config.embedding_path = os.path.join(config.data_dir, 'glove.trimmed.100.npz')\n",
        "\n",
        "config.context_len = 600\n",
        "config.question_len = 30\n",
        "\n",
        "config.hidden_dim = 200\n",
        "config.embedding_size=100\n",
        "\n",
        "#vector with zeros for unknown words\n",
        "config.max_dec_steps = 4\n",
        "config.maxout_pool_size=16\n",
        "\n",
        "config.lr = 0.001\n",
        "config.dropout_ratio = 0.15\n",
        "\n",
        "config.max_grad_norm = 5.0\n",
        "config.batch_size = 100\n",
        "config.num_epochs = 50\n",
        "\n",
        "config.print_every = 100\n",
        "config.save_every = 50000000\n",
        "config.eval_every = 1000\n",
        "\n",
        "config.model_type = 'baseline'\n",
        "config.reg_lambda = 0.00007\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1KoHKzb4unv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_path = os.path.join(config.data_dir, \"glove.6B.{}d.txt\".format(config.embedding_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBjph0hNBpYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Processor(object):\n",
        "    def __init__(self):\n",
        "        self.glove_path = os.path.join(config.data_dir, \"glove.6B.{}d.txt\".format(config.embedding_size))\n",
        "        self.emb_matrix, self.word2id, self.id2word = get_glove(self.glove_path, config.embedding_size)\n",
        "\n",
        "        self.train_context_path = os.path.join(config.data_dir, \"train.tweet\")\n",
        "        self.train_qn_path = os.path.join(config.data_dir, \"train.question\")\n",
        "        self.train_ans_path = os.path.join(config.data_dir, \"train.span\")\n",
        "        self.dev_context_path = os.path.join(config.data_dir, \"dev.tweet\")\n",
        "        self.dev_qn_path = os.path.join(config.data_dir, \"dev.question\")\n",
        "        self.dev_ans_path = os.path.join(config.data_dir, \"dev.span\")\n",
        "\n",
        "    def get_mask_from_seq_len(self, seq_mask):\n",
        "        seq_lens = np.sum(seq_mask, 1)\n",
        "        max_len = np.max(seq_lens)\n",
        "        indices = np.arange(0, max_len)\n",
        "        mask = (indices < np.expand_dims(seq_lens, 1)).astype(int)\n",
        "        return mask\n",
        "\n",
        "    def get_data(self, batch, is_train=True):\n",
        "        qn_mask = self.get_mask_from_seq_len(batch.qn_mask)\n",
        "        qn_mask_var = torch.from_numpy(qn_mask).long()\n",
        "\n",
        "        context_mask = self.get_mask_from_seq_len(batch.context_mask)\n",
        "        context_mask_var = torch.from_numpy(context_mask).long()\n",
        "\n",
        "        qn_seq_var = torch.from_numpy(batch.qn_ids).long()\n",
        "        context_seq_var = torch.from_numpy(batch.context_ids).long()\n",
        "\n",
        "        if is_train:\n",
        "            span_var = torch.from_numpy(batch.ans_span).long()\n",
        "\n",
        "        if use_cuda:\n",
        "            qn_mask_var = qn_mask_var.cuda()\n",
        "            context_mask_var = context_mask_var.cuda()\n",
        "            qn_seq_var = qn_seq_var.cuda()\n",
        "            context_seq_var = context_seq_var.cuda()\n",
        "            if is_train:\n",
        "                span_var = span_var.cuda()\n",
        "\n",
        "        if is_train:\n",
        "            return qn_seq_var, qn_mask_var, context_seq_var, context_mask_var, span_var\n",
        "        else:\n",
        "            return qn_seq_var, qn_mask_var, context_seq_var, context_mask_var\n",
        "\n",
        "    def save_model(self, model, optimizer, loss, global_step, epoch, model_dir):\n",
        "        model_state = model.state_dict()\n",
        "        model_state = {k: v for k, v in model_state.items() if 'embedding' not in k}\n",
        "\n",
        "        state = {\n",
        "            'global_step': global_step,\n",
        "            'epoch': epoch,\n",
        "            'model': model_state,\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'current_loss': loss\n",
        "        }\n",
        "        model_save_path = os.path.join(model_dir, 'model_%d_%d_%d' % (global_step, epoch, int(time.time())))\n",
        "        torch.save(state, model_save_path)\n",
        "\n",
        "    def get_model(self, model_file_path=None, is_eval=False):\n",
        "        if config.model_type == 'co-attention':\n",
        "            model = CoattentionModel(config.hidden_dim, config.maxout_pool_size,\n",
        "                                 self.emb_matrix, config.max_dec_steps, config.dropout_ratio)\n",
        "        else:\n",
        "            model = Baseline(config.hidden_dim, self.emb_matrix, config.dropout_ratio)\n",
        "\n",
        "        if is_eval:\n",
        "            model = model.eval()\n",
        "        if use_cuda:\n",
        "            model = model.cuda()\n",
        "\n",
        "        if model_file_path is not None:\n",
        "            state = torch.load(model_file_path, map_location=lambda storage, location: storage)\n",
        "            model.load_state_dict(state['model'], strict=False)\n",
        "\n",
        "        return model\n",
        "    def get_grad_norm(self, parameters, norm_type=2):\n",
        "        parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
        "        total_norm = 0\n",
        "        for p in parameters:\n",
        "            param_norm = p.grad.data.norm(norm_type)\n",
        "            total_norm += param_norm ** norm_type\n",
        "        total_norm = total_norm ** (1. / norm_type)\n",
        "        return total_norm\n",
        "\n",
        "    def get_param_norm(self, parameters, norm_type=2):\n",
        "        total_norm = 0\n",
        "        for p in parameters:\n",
        "            param_norm = p.data.norm(norm_type)\n",
        "            total_norm += param_norm ** norm_type\n",
        "        total_norm = total_norm ** (1. / norm_type)\n",
        "        return total_norm\n",
        "\n",
        "    def train_one_batch(self, batch, model, optimizer, params):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        q_seq, q_lens, d_seq, d_lens, span = self.get_data(batch)\n",
        "        loss, _, _ = model(q_seq, q_lens, d_seq, d_lens, span)\n",
        "\t\n",
        "\tl2_reg = None\n",
        "\tfor W in params:\n",
        "    \t    if l2_reg is None:\n",
        "        \tl2_reg = W.norm(2)\n",
        "    \t    else:\n",
        "        \tl2_reg = l2_reg + W.norm(2)\n",
        "\tloss = loss + config.reg_lambda * l2_reg\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        param_norm = self.get_param_norm(params)\n",
        "        grad_norm = self.get_grad_norm(params)\n",
        "\n",
        "        clip_grad_norm_(params, config.max_grad_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item(), param_norm, grad_norm\n",
        "\n",
        "    def eval_one_batch(self, batch, model):\n",
        "        model.eval()\n",
        "        q_seq, q_lens, d_seq, d_lens, span = self.get_data(batch)\n",
        "        loss, pred_start_pos, pred_end_pos = model(q_seq, q_lens, d_seq, d_lens, span)\n",
        "        return loss.item(), pred_start_pos.data, pred_end_pos.data\n",
        "\n",
        "    def test_one_batch(self, batch, model):\n",
        "        model.eval()\n",
        "        q_seq, q_lens, d_seq, d_lens = self.get_data(batch, is_train=False)\n",
        "        pred_start_pos, pred_end_pos = model(q_seq, q_lens, d_seq, d_lens)\n",
        "        return pred_start_pos.data, pred_end_pos.data\n",
        "\n",
        "    def train(self, model_file_path):\n",
        "        train_dir = os.path.join(config.log_root, 'train_%d' % (int(time.time())))\n",
        "        if not os.path.exists(train_dir):\n",
        "            os.mkdir(train_dir)\n",
        "        model_dir = os.path.join(train_dir, 'model')\n",
        "        if not os.path.exists(model_dir):\n",
        "            os.mkdir(model_dir)\n",
        "        bestmodel_dir = os.path.join(train_dir, 'bestmodel')\n",
        "        if not os.path.exists(bestmodel_dir):\n",
        "           os.makedirs(bestmodel_dir)\n",
        "\n",
        "        summary_writer = tf.summary.FileWriter(train_dir)\n",
        "\n",
        "        with open(os.path.join(train_dir, \"flags.json\"), 'w') as fout:\n",
        "            json.dump(vars(config), fout)\n",
        "\n",
        "        model = self.get_model(model_file_path)\n",
        "        params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "        optimizer = Adam(params, lr=config.lr, amsgrad=True)\n",
        "\n",
        "        num_params = sum(p.numel() for p in params)\n",
        "        logging.info(\"Number of params: %d\" % num_params)\n",
        "\n",
        "        exp_loss, best_dev_f1, best_dev_em = None, None, None\n",
        "\n",
        "        epoch = 0\n",
        "        global_step = 0\n",
        "\n",
        "        logging.info(\"Beginning training loop...\")\n",
        "        while config.num_epochs == 0 or epoch < config.num_epochs:\n",
        "            epoch += 1\n",
        "            epoch_tic = time.time()\n",
        "            for batch in get_batch_generator(self.word2id, self.train_context_path,\n",
        "                                             self.train_qn_path, self.train_ans_path,\n",
        "                                             config.batch_size, context_len=config.context_len,\n",
        "                                             question_len=config.question_len, discard_long=True):\n",
        "                global_step += 1\n",
        "                iter_tic = time.time()\n",
        "\n",
        "                loss, param_norm, grad_norm = self.train_one_batch(batch, model, optimizer, params)\n",
        "                write_summary(loss, \"train/loss\", summary_writer, global_step)\n",
        "\n",
        "                iter_toc = time.time()\n",
        "                iter_time = iter_toc - iter_tic\n",
        "\n",
        "                if not exp_loss:\n",
        "                    exp_loss = loss\n",
        "                else:\n",
        "                    exp_loss = 0.99 * exp_loss + 0.01 * loss\n",
        "\n",
        "                if global_step % config.print_every == 0:\n",
        "                    logging.info(\n",
        "                        'epoch %d, iter %d, loss %.5f, smoothed loss %.5f, grad norm %.5f, param norm %.5f, batch time %.3f' %\n",
        "                        (epoch, global_step, loss, exp_loss, grad_norm, param_norm, iter_time))\n",
        "\n",
        "\n",
        "                if global_step % config.save_every == 0:\n",
        "                    logging.info(\"Saving to %s...\" % model_dir)\n",
        "                    self.save_model(model, optimizer, loss, global_step, epoch, model_dir)\n",
        "\n",
        "                if global_step % config.eval_every == 0:\n",
        "                    dev_loss = self.get_dev_loss(model)\n",
        "                    logging.info(\"Epoch %d, Iter %d, dev loss: %f\" % (epoch, global_step, dev_loss))\n",
        "                    write_summary(dev_loss, \"dev/loss\", summary_writer, global_step)\n",
        "\n",
        "                    train_f1, train_em = self.check_f1_em(model, \"train\", num_samples=1000)\n",
        "                    logging.info(\"Epoch %d, Iter %d, Train F1 score: %f, Train EM score: %f\" % (\n",
        "                        epoch, global_step, train_f1, train_em))\n",
        "                    write_summary(train_f1, \"train/F1\", summary_writer, global_step)\n",
        "                    write_summary(train_em, \"train/EM\", summary_writer, global_step)\n",
        "\n",
        "                    dev_f1, dev_em = self.check_f1_em(model, \"dev\", num_samples=0)\n",
        "                    logging.info(\n",
        "                        \"Epoch %d, Iter %d, Dev F1 score: %f, Dev EM score: %f\" % (epoch, global_step, dev_f1, dev_em))\n",
        "                    write_summary(dev_f1, \"dev/F1\", summary_writer, global_step)\n",
        "                    write_summary(dev_em, \"dev/EM\", summary_writer, global_step)\n",
        "\n",
        "                    if best_dev_f1 is None or dev_f1 > best_dev_f1:\n",
        "                        best_dev_f1 = dev_f1\n",
        "\n",
        "                    if best_dev_em is None or dev_em > best_dev_em:\n",
        "                        best_dev_em = dev_em\n",
        "                        logging.info(\"Saving to %s...\" % bestmodel_dir)\n",
        "                        self.save_model(model, optimizer, loss, global_step, epoch, bestmodel_dir)\n",
        "\n",
        "\n",
        "            epoch_toc = time.time()\n",
        "            logging.info(\"End of epoch %i. Time for epoch: %f\" % (epoch, epoch_toc - epoch_tic))\n",
        "\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    def check_f1_em(self, model, dataset, num_samples=100, print_to_screen=False):\n",
        "        logging.info(\"Calculating F1/EM for %s examples in %s set...\" % (str(num_samples) if num_samples != 0 else \"all\", dataset))\n",
        "        # print(\"Calculating F1/EM for %s examples in %s set...\" % (str(num_samples) if num_samples != 0 else \"all\", dataset))\n",
        "\n",
        "        if dataset == \"train\":\n",
        "            context_path, qn_path, ans_path = self.train_context_path, self.train_qn_path, self.train_ans_path\n",
        "        elif dataset == \"dev\":\n",
        "            context_path, qn_path, ans_path = self.dev_context_path, self.dev_qn_path, self.dev_ans_path\n",
        "        else:\n",
        "            raise ('dataset is not defined')\n",
        "        print(context_path)\n",
        "        f1_total = 0.\n",
        "        em_total = 0.\n",
        "        example_num = 1\n",
        "\n",
        "        tic = time.time()\n",
        "\n",
        "        for batch in get_batch_generator(self.word2id, context_path, qn_path, ans_path, config.batch_size,\n",
        "                                         context_len=config.context_len, question_len=config.question_len,\n",
        "                                         discard_long=False):\n",
        "\n",
        "            pred_start_pos, pred_end_pos = self.test_one_batch(batch, model)\n",
        "\n",
        "            pred_start_pos = pred_start_pos.tolist()\n",
        "            pred_end_pos = pred_end_pos.tolist()\n",
        "\n",
        "            for ex_idx, (pred_ans_start, pred_ans_end, true_ans_tokens) \\\n",
        "                    in enumerate(zip(pred_start_pos, pred_end_pos, batch.ans_tokens)):\n",
        "                example_num += 1\n",
        "                pred_ans_tokens = batch.context_tokens[ex_idx][pred_ans_start : pred_ans_end + 1]\n",
        "                pred_answer = \" \".join(pred_ans_tokens)\n",
        "\n",
        "                true_answer = \" \".join(true_ans_tokens)\n",
        "\n",
        "                f1 = f1_score(pred_answer, true_answer)\n",
        "                em = exact_match_score(pred_answer, true_answer)\n",
        "                f1_total += f1\n",
        "                em_total += em\n",
        "\n",
        "                if print_to_screen:\n",
        "                    print_example(self.word2id, batch.context_tokens[ex_idx], batch.qn_tokens[ex_idx],\n",
        "                                  batch.ans_span[ex_idx, 0], batch.ans_span[ex_idx, 1], pred_ans_start,\n",
        "                                  pred_ans_end, true_answer, pred_answer, f1, em)\n",
        "\n",
        "                if num_samples != 0 and example_num >= num_samples:\n",
        "                    break\n",
        "\n",
        "            if num_samples != 0 and example_num >= num_samples:\n",
        "                break\n",
        "\n",
        "        f1_total /= example_num\n",
        "        em_total /= example_num\n",
        "\n",
        "        toc = time.time()\n",
        "        logging.info(\"Calculating F1/EM for %i examples in %s set took %.2f seconds\" % (example_num, dataset, toc-tic))\n",
        "\n",
        "        return f1_total, em_total\n",
        "\n",
        "    def get_dev_loss(self, model):\n",
        "        logging.info(\"Calculating dev loss...\")\n",
        "        tic = time.time()\n",
        "        loss_per_batch, batch_lengths = [], []\n",
        "        i = 0\n",
        "        for batch in get_batch_generator(self.word2id, self.dev_context_path, self.dev_qn_path, self.dev_ans_path,\n",
        "                                         config.batch_size, context_len=config.context_len,\n",
        "                                         question_len=config.question_len, discard_long=True):\n",
        "\n",
        "            loss, _, _ = self.eval_one_batch(batch, model)\n",
        "            curr_batch_size = batch.batch_size\n",
        "            loss_per_batch.append(loss * curr_batch_size)\n",
        "            batch_lengths.append(curr_batch_size)\n",
        "            i += 1\n",
        "            if i == 10:\n",
        "                break\n",
        "        total_num_examples = sum(batch_lengths)\n",
        "        toc = time.time()\n",
        "        print (\"Computed dev loss over %i examples in %.2f seconds\" % (total_num_examples, toc-tic))\n",
        "\n",
        "        dev_loss = sum(loss_per_batch) / float(total_num_examples)\n",
        "\n",
        "        return dev_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QLAKZmzCV2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_summary(value, tag, summary_writer, global_step):\n",
        "    summary = tf.Summary()\n",
        "    summary.value.add(tag=tag, simple_value=value)\n",
        "    summary_writer.add_summary(summary, global_step)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zHP2lt9Cnei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_file_path = path+'/squad_trained_model/model_9000_11_1574846039'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-q4lwrBkcc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "argv = [\"train\",model_file_path]\n",
        "mode = argv[0]\n",
        "processor = Processor()\n",
        "if mode == \"train\":\n",
        "    model_file_path = None\n",
        "    if len(argv) > 1:\n",
        "        model_file_path = argv[1]\n",
        "    processor.train(model_file_path)\n",
        "elif mode == \"show_examples\":\n",
        "    print(\"in here\")\n",
        "    model_file_path = argv[1]\n",
        "    model = processor.get_model(model_file_path)\n",
        "    processor.check_f1_em(model,'train', num_samples=10, print_to_screen=False)\n",
        "\n",
        "elif mode == \"official_eval\":\n",
        "    model_file_path = argv[1]\n",
        "    json_in_path = \"\"\n",
        "    json_out_path = \"\"\n",
        "    qn_uuid_data, context_token_data, qn_token_data = get_json_data(json_in_path)\n",
        "    model = processor.get_model(model_file_path)\n",
        "    answers_dict = generate_answers(config, model, processor, qn_uuid_data, context_token_data, qn_token_data)\n",
        "    print (\"Writing predictions to %s...\" % json_out_path)\n",
        "    with io.open(json_out_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(unicode(json.dumps(answers_dict, ensure_ascii=False)))\n",
        "        print (\"Wrote predictions to %s\" % json_out_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od838Xok5Czy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "mode = \"show_examples\"\n",
        "model_file_path = argv[1]\n",
        "model = processor.get_model(model_file_path)\n",
        "processor.check_f1_em(model,'train', num_samples=100, print_to_screen=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aYv1oxEHNwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "json_in_path = \"/content/gdrive/My Drive/co-attention/co-attention/data/dev-v1.1.json\"\n",
        "json_out_path = \"/content/gdrive/My Drive/co-attention/co-attention/data/jsonout/out.json\"\n",
        "qn_uuid_data, context_token_data, qn_token_data = get_json_data(json_in_path)\n",
        "model = processor.get_model(model_file_path)\n",
        "answers_dict = generate_answers(config, model, processor, qn_uuid_data, context_token_data, qn_token_data)\n",
        "print (\"Writing predictions to %s...\" % json_out_path)\n",
        "with io.open(json_out_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(unicode(json.dumps(answers_dict, ensure_ascii=False)))\n",
        "        print (\"Wrote predictions to %s\" % json_out_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHO6ctqQ50uF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_file_path = '/co-attention/log/bestmodel/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNWO7VSJ5-QL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = processor.get_model(model_file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}